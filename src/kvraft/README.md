# Lab 3A

## Block point:

1. If you plan to transfer the result by channel, you need to make sure that the channel exists during the transfer and close the channel after the RPC.

# Lab 3B

## Block point:

1. When should you snapshot?
   We can't take snapshot just to compare the state size with the max allowing size after every command, since Raft's last log index may be greatly larger than the service applied index, which will cause it to take snapshot every command. How about taking snapshot after a period of time? It will work, but the state size may be beyond the test size within some periods of time. Therefore, I think the best way to snapshot is after a period of time or after a number of applied commands.
2. What should be persisted?
   The most important thing is the data and duplicated table, besides we also need to persist the lastIncludedIndex. Image when the server crashes, if we don't persist the lastIncludedIndex, and the next command is an outdated snapshot when the server restarts, the server may applied this snapshot since the lastIncludedIndex is 0 now.

## Test result:

```
Test: one client (3A) ...
labgob warning: Decoding into a non-default variable/field Err may not work
  ... Passed --  15.1  5 35275 7051
Test: ops complete fast enough (3A) ...
  ... Passed --   1.0  3  3017    0
Test: many clients (3A) ...
  ... Passed --  15.1  5 26061 9079
Test: unreliable net, many clients (3A) ...
  ... Passed --  15.9  5 10034 1767
Test: concurrent append to same key, unreliable (3A) ...
  ... Passed --   1.1  3   199   52
Test: progress in majority (3A) ...
  ... Passed --   0.6  5    68    2
Test: no progress in minority (3A) ...
  ... Passed --   1.0  5   102    3
Test: completion after heal (3A) ...
  ... Passed --   1.0  5    56    3
Test: partitions, one client (3A) ...
  ... Passed --  22.8  5 24876 4786
Test: partitions, many clients (3A) ...
  ... Passed --  22.3  5 79828 7307
Test: restarts, one client (3A) ...
  ... Passed --  21.8  5 79038 6739
Test: restarts, many clients (3A) ...
  ... Passed --  23.0  5 206946 9075
Test: unreliable net, restarts, many clients (3A) ...
  ... Passed --  23.0  5 11069 1722
Test: restarts, partitions, many clients (3A) ...
  ... Passed --  27.5  5 119099 6802
Test: unreliable net, restarts, partitions, many clients (3A) ...
  ... Passed --  28.8  5  8600 1163
Test: unreliable net, restarts, partitions, random keys, many clients (3A) ...
  ... Passed --  31.0  7 27516 2736
Test: InstallSnapshot RPC (3B) ...
  ... Passed --   2.5  3   318   63
Test: snapshot size is reasonable (3B) ...
  ... Passed --   0.7  3  2411  800
Test: ops complete fast enough (3B) ...
  ... Passed --   0.8  3  3054    0
Test: restarts, snapshots, one client (3B) ...
  ... Passed --  25.8  5 382570 69024
Test: restarts, snapshots, many clients (3B) ...
  ... Passed --  23.4  5 669609 137057
Test: unreliable net, snapshots, many clients (3B) ...
  ... Passed --  15.7  5 11484 1834
Test: unreliable net, restarts, snapshots, many clients (3B) ...
  ... Passed --  22.7  5 12895 1872
Test: unreliable net, restarts, partitions, snapshots, many clients (3B) ...
  ... Passed --  28.5  5  8465 1156
Test: unreliable net, restarts, partitions, snapshots, random keys, many clients (3B) ...
  ... Passed --  29.2  7 36682 3614
PASS
ok  	6.5840/kvraft	401.111s
```

## Later work:

1. In my implementation, the leader sends snapshot to the follower when follower's log is just one index behind and the follower is going to take a snapshot. So it is better to allow the follower's log lag a few logs behind the leader.
