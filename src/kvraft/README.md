# Lab 3A

## Block point:

1. If you plan to transfer the result by channel, you need to make sure that the channel exists during the transfer and close the channel after the RPC.

# Lab 3B

## Block point:

1. When should you snapshot?
   We can't take snapshot just to compare the state size with the max allowing size after every command, since Raft's last log index may be greatly larger than the service applied index, which will cause it to take snapshot every command. How about taking snapshot after a period of time? It will work, but the state size may be beyond the test size within some periods of time. Therefore, I think the best way to snapshot is after a period of time or after a number of applied commands.
2. What should be persisted?
   The most important thing is the data and duplicated table, besides we also need to persist the lastIncludedIndex. Image when the server crashes, if we don't persist the lastIncludedIndex, and the next command is an outdated snapshot when the server restarts, the server may applied this snapshot since the lastIncludedIndex is 0 now.

## Test result:

Environment:

13â€‘inch M2 MacBook Pro,  8GB unified memory,  256GB SSD storage.

```
Test: one client (3A) ...
labgob warning: Decoding into a non-default variable/field Err may not work
  ... Passed --  15.1  5 33315 6654
Test: ops complete fast enough (3A) ...
  ... Passed --   1.1  3  3030    0
Test: many clients (3A) ...
  ... Passed --  15.1  5 23392 8303
Test: unreliable net, many clients (3A) ...
  ... Passed --  16.0  5  8003 1373
Test: concurrent append to same key, unreliable (3A) ...
  ... Passed --   1.9  3   229   52
Test: progress in majority (3A) ...
  ... Passed --   0.6  5    52    2
Test: no progress in minority (3A) ...
  ... Passed --   1.0  5   104    3
Test: completion after heal (3A) ...
  ... Passed --   1.0  5    53    3
Test: partitions, one client (3A) ...
  ... Passed --  22.4  5 24241 4704
Test: partitions, many clients (3A) ...
  ... Passed --  22.8  5 19688 6377
Test: restarts, one client (3A) ...
  ... Passed --  20.2  5 30595 6065
Test: restarts, many clients (3A) ...
  ... Passed --  20.2  5 23871 8346
Test: unreliable net, restarts, many clients (3A) ...
  ... Passed --  21.5  5  8369 1327
Test: restarts, partitions, many clients (3A) ...
  ... Passed --  27.2  5 22942 7505
Test: unreliable net, restarts, partitions, many clients (3A) ...
  ... Passed --  28.8  5  6115  799
Test: unreliable net, restarts, partitions, random keys, many clients (3A) ...
  ... Passed --  30.4  7 18719 1627
Test: InstallSnapshot RPC (3B) ...
  ... Passed --   3.4  3   334   63
Test: snapshot size is reasonable (3B) ...
  ... Passed --   0.7  3  2534  800
Test: ops complete fast enough (3B) ...
  ... Passed --   0.8  3  3029    0
Test: restarts, snapshots, one client (3B) ...
info: linearizability check timed out, assuming history is ok
  ... Passed --  23.4  5 295753 58896
Test: restarts, snapshots, many clients (3B) ...
  ... Passed --  20.4  5 386581 110351
Test: unreliable net, snapshots, many clients (3B) ...
  ... Passed --  16.1  5  8992 1419
Test: unreliable net, restarts, snapshots, many clients (3B) ...
  ... Passed --  21.0  5  9910 1487
Test: unreliable net, restarts, partitions, snapshots, many clients (3B) ...
  ... Passed --  29.9  5  6917  854
Test: unreliable net, restarts, partitions, snapshots, random keys, many clients (3B) ...
  ... Passed --  31.4  7 28921 2806
PASS
ok  	6.5840/kvraft	393.188s
```

## Later work:

1. In my implementation, the leader sends snapshot to the follower when follower's log is just one index behind and the follower is going to take a snapshot. So it is better to allow the follower's log lag a few logs behind the leader.
