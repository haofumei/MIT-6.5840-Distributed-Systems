# Lab 3A

## Block point:

1. If you plan to transfer the result by channel, you need to make sure that the channel exists during the transfer and close the channel after the RPC.

# Lab 3B

## Block point:

1. if you try to just compare the maxraftstate with persister.RaftStateSize(), and trigger the snapshot, it may cause a problem. Image when server is accepting commands from applyCh, Raft is also starting new agreements at the same time. Assuming index 20 at Raft'log is a threshold to trigger the snapshot, server 1 and 2 may choose to snapshot at index 19 or 20, because of the delays of accepting commands from applyCh. Thus, setting a checkpoint(how many logs can we trigger a snapshot) and calculating the ratio of maxraftstate and persister.RaftStateSize() should be a better option.
2. What should be persisted?

## Test result:

```
Test: one client (3A) ...
labgob warning: Decoding into a non-default variable/field Err may not work
  ... Passed --  15.1  5 35275 7051
Test: ops complete fast enough (3A) ...
  ... Passed --   1.0  3  3017    0
Test: many clients (3A) ...
  ... Passed --  15.1  5 26061 9079
Test: unreliable net, many clients (3A) ...
  ... Passed --  15.9  5 10034 1767
Test: concurrent append to same key, unreliable (3A) ...
  ... Passed --   1.1  3   199   52
Test: progress in majority (3A) ...
  ... Passed --   0.6  5    68    2
Test: no progress in minority (3A) ...
  ... Passed --   1.0  5   102    3
Test: completion after heal (3A) ...
  ... Passed --   1.0  5    56    3
Test: partitions, one client (3A) ...
  ... Passed --  22.8  5 24876 4786
Test: partitions, many clients (3A) ...
  ... Passed --  22.3  5 79828 7307
Test: restarts, one client (3A) ...
  ... Passed --  21.8  5 79038 6739
Test: restarts, many clients (3A) ...
  ... Passed --  23.0  5 206946 9075
Test: unreliable net, restarts, many clients (3A) ...
  ... Passed --  23.0  5 11069 1722
Test: restarts, partitions, many clients (3A) ...
  ... Passed --  27.5  5 119099 6802
Test: unreliable net, restarts, partitions, many clients (3A) ...
  ... Passed --  28.8  5  8600 1163
Test: unreliable net, restarts, partitions, random keys, many clients (3A) ...
  ... Passed --  31.0  7 27516 2736
Test: InstallSnapshot RPC (3B) ...
  ... Passed --   2.5  3   318   63
Test: snapshot size is reasonable (3B) ...
  ... Passed --   0.7  3  2411  800
Test: ops complete fast enough (3B) ...
  ... Passed --   0.8  3  3054    0
Test: restarts, snapshots, one client (3B) ...
  ... Passed --  25.8  5 382570 69024
Test: restarts, snapshots, many clients (3B) ...
  ... Passed --  23.4  5 669609 137057
Test: unreliable net, snapshots, many clients (3B) ...
  ... Passed --  15.7  5 11484 1834
Test: unreliable net, restarts, snapshots, many clients (3B) ...
  ... Passed --  22.7  5 12895 1872
Test: unreliable net, restarts, partitions, snapshots, many clients (3B) ...
  ... Passed --  28.5  5  8465 1156
Test: unreliable net, restarts, partitions, snapshots, random keys, many clients (3B) ...
  ... Passed --  29.2  7 36682 3614
PASS
ok  	6.5840/kvraft	401.111s
```

## Later work:

1. In my implementation, the leader sends snapshot to the follower when follower's log is just one index behind and the follower is going to take a snapshot. So it is better to allow the follower's log lag a few logs behind the leader.
2. The number of RPC need to be optimized.
